# The following code is largely borrowed from
# https://github.com/facebookresearch/detectron2/blob/master/demo/demo.py and
# https://github.com/facebookresearch/detectron2/blob/master/demo/predictor.py

from typing import Optional
import argparse
import torch
import numpy as np
import time

from detectron2.config import get_cfg
from detectron2.utils.logger import setup_logger
from detectron2.data.catalog import MetadataCatalog
from detectron2.modeling import build_model
from detectron2.checkpoint import DetectionCheckpointer
from detectron2.utils.visualizer import ColorMode, Visualizer

from submission.utils.constants import coco_categories_mapping, coco_categories


class MaskRCNN:
    def __init__(self, sem_pred_prob_thr: float, sem_gpu_id: int, visualize: bool):
        self.segmentation_model = ImageSegmentation(sem_pred_prob_thr, sem_gpu_id)
        self.visualize = visualize
        self.num_sem_categories = len(coco_categories)

    # def get_prediction(self, img, depth=None):
    #     image_list = []
    #     img = img[:, :, ::-1]
    #     image_list.append(img)
    #     seg_predictions, vis_output = self.segmentation_model.get_predictions(
    #         image_list, visualize=self.visualize
    #     )
    #
    #     semantic_pred = np.zeros((img.shape[0], img.shape[1], self.num_sem_categories))
    #
    #     for j, class_idx in enumerate(seg_predictions[0]["instances"].pred_classes.cpu().numpy()):
    #         if class_idx in list(coco_categories_mapping.keys()):
    #             idx = coco_categories_mapping[class_idx]
    #             obj_mask = seg_predictions[0]["instances"].pred_masks[j] * 1.0
    #             obj_mask = obj_mask.cpu().numpy()
    #
    #             if depth is not None:
    #                 md = np.median(depth[obj_mask == 1])
    #                 if md == 0:
    #                     filter_mask = np.ones_like(obj_mask, dtype=bool)
    #                 else:
    #                     # Restrict objects to 2m depth
    #                     filter_mask = (depth >= md + 50) | (depth <= md - 50)
    #                 # print(
    #                 #     f"Median object depth: {md.item()}, filtering out {np.count_nonzero(filter_mask)} pixels"
    #                 # )
    #                 obj_mask[filter_mask] = 0.0
    #
    #             semantic_pred[:, :, idx] += obj_mask
    #
    #     if self.visualize:
    #         img = vis_output.get_image()
    #
    #     return semantic_pred, img

    def get_prediction(self,
                       images: np.ndarray,
                       depths: Optional[np.ndarray] = None):
        """
        Arguments:
            images: image frames of shape (batch_size, H, W, 3)
            depths: depth frames of shape (batch_size, H, W)
        """
        batch_size, height, width, _ = images.shape
        images = images[:, :, :, ::-1]

        predictions, vis_outputs = self.segmentation_model.get_predictions(
            images, visualize=self.visualize
        )
        one_hot_predictions = np.zeros((batch_size, height, width, self.num_sem_categories))

        for i in range(batch_size):
            for j, class_idx in enumerate(predictions[i]["instances"].pred_classes.cpu().numpy()):
                if class_idx in list(coco_categories_mapping.keys()):
                    idx = coco_categories_mapping[class_idx]
                    obj_mask = predictions[i]["instances"].pred_masks[j] * 1.0
                    obj_mask = obj_mask.cpu().numpy()

                    if depths is not None:
                        depth = depths[i]
                        md = np.median(depth[obj_mask == 1])
                        if md == 0:
                            filter_mask = np.ones_like(obj_mask, dtype=bool)
                        else:
                            # Restrict objects to 2m depth
                            filter_mask = (depth >= md + 50) | (depth <= md - 50)
                        # print(
                        #     f"Median object depth: {md.item()}, filtering out {np.count_nonzero(filter_mask)} pixels"
                        # )
                        obj_mask[filter_mask] = 0.0

                    one_hot_predictions[i, :, :, idx] += obj_mask

        if self.visualize:
            # TODO Replace RGB with visualization
            pass
            # img = vis_output.get_image()

        return one_hot_predictions, images


class ImageSegmentation:
    def __init__(self, sem_pred_prob_thr, sem_gpu_id):
        string_args = f"""
            --config-file submission/obs_preprocessor/mask_rcnn_R_50_FPN_3x.yaml
            --input input1.jpeg
            --confidence-threshold {sem_pred_prob_thr}
            --opts MODEL.WEIGHTS
            detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl
            """

        if sem_gpu_id == -1:
            string_args += """ MODEL.DEVICE cpu"""
        else:
            string_args += f""" MODEL.DEVICE cuda:{sem_gpu_id}"""

        string_args = string_args.split()

        args = get_seg_parser().parse_args(string_args)
        logger = setup_logger()
        logger.info("Arguments: " + str(args))

        cfg = setup_cfg(args)
        self.demo = VisualizationDemo(cfg)

    def get_predictions(self, images, visualize=False):
        return self.demo.run_on_image(images, visualize=visualize)


def setup_cfg(args):
    # load config from file and command-line arguments
    cfg = get_cfg()
    cfg.merge_from_file(args.config_file)
    cfg.merge_from_list(args.opts)
    # Set score_threshold for builtin models
    cfg.MODEL.RETINANET.SCORE_THRESH_TEST = args.confidence_threshold
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = args.confidence_threshold
    cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = args.confidence_threshold
    cfg.freeze()
    return cfg


def get_seg_parser():
    parser = argparse.ArgumentParser(description="Detectron2 demo for builtin models")
    parser.add_argument(
        "--config-file",
        default="configs/quick_schedules/mask_rcnn_R_50_FPN_inference_acc_test.yaml",
        metavar="FILE",
        help="path to config file",
    )
    parser.add_argument("--webcam", action="store_true", help="Take inputs from webcam.")
    parser.add_argument("--video-input", help="Path to video file.")
    parser.add_argument("--input", nargs="+", help="A list of space separated input images")
    parser.add_argument(
        "--output",
        help="A file or directory to save output visualizations. "
        "If not given, will show output in an OpenCV window.",
    )
    parser.add_argument(
        "--confidence-threshold",
        type=float,
        default=0.5,
        help="Minimum score for instance predictions to be shown",
    )
    parser.add_argument(
        "--opts",
        help="Modify config options using the command-line 'KEY VALUE' pairs",
        default=[],
        nargs=argparse.REMAINDER,
    )
    return parser


class VisualizationDemo(object):
    def __init__(self, cfg, instance_mode=ColorMode.IMAGE):
        """
        Args:
            cfg (CfgNode):
            instance_mode (ColorMode):
        """
        self.metadata = MetadataCatalog.get(
            cfg.DATASETS.TEST[0] if len(cfg.DATASETS.TEST) else "__unused"
        )
        self.cpu_device = torch.device("cpu")
        self.instance_mode = instance_mode

        self.predictor = BatchPredictor(cfg)

    def run_on_image(self, images, visualize=0):
        """
        Args:
            images (np.ndarray): image of shape (batch_size, H, W, C)
             (in BGR order). This is the format used by OpenCV.
        Returns:
            all_predictions (dict): the output of the model
            all_vis_output (VisImage): the visualized image output
        """
        all_predictions = self.predictor(images)
        all_vis_outputs = []

        if visualize:
            for i in range(all_predictions.shape[0]):
                predictions = all_predictions[i]
                image = images[i]
                visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)
                if "panoptic_seg" in predictions:
                    panoptic_seg, segments_info = predictions["panoptic_seg"]
                    vis_output = visualizer.draw_panoptic_seg_predictions(
                        panoptic_seg.to(self.cpu_device), segments_info
                    )
                else:
                    if "sem_seg" in predictions:
                        vis_output = visualizer.draw_sem_seg(
                            predictions["sem_seg"].argmax(dim=0).to(self.cpu_device)
                        )
                    if "instances" in predictions:
                        instances = predictions["instances"].to(self.cpu_device)
                        vis_output = visualizer.draw_instance_predictions(predictions=instances)
                all_vis_outputs.append(vis_output)

        return all_predictions, all_vis_outputs


class BatchPredictor:
    """
    Create a simple end-to-end predictor with the given config that runs on
    single device for a list of input images.
    Compared to using the model directly, this class does the following
    additions:
    1. Load checkpoint from `cfg.MODEL.WEIGHTS`.
    2. Always take BGR image as the input and apply conversion defined by
         `cfg.INPUT.FORMAT`.
    3. Apply resizing defined by `cfg.INPUT.{MIN,MAX}_SIZE_TEST`.
    4. Take a list of input images
    Attributes:
        metadata (Metadata): the metadata of the underlying dataset, obtained
            from cfg.DATASETS.TEST.
    """

    def __init__(self, cfg):
        self.cfg = cfg.clone()  # cfg can be modified by model
        self.model = build_model(self.cfg)
        self.model.eval()
        self.metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])

        checkpointer = DetectionCheckpointer(self.model)
        checkpointer.load(cfg.MODEL.WEIGHTS)

        self.input_format = cfg.INPUT.FORMAT
        assert self.input_format in ["RGB", "BGR"], self.input_format

    def __call__(self, image_list):
        """
        Args:
            image_list (list of np.ndarray): a list of images of
                                             shape (H, W, C) (in BGR order).
        Returns:
            predictions (dict):
                the output of the model for all images.
                See :doc:`/tutorials/models` for details about the format.
        """
        inputs = []
        for original_image in image_list:
            # https://github.com/sphinx-doc/sphinx/issues/4258
            # Apply pre-processing to image.
            if self.input_format == "RGB":
                # whether the model expects BGR inputs or RGB
                original_image = original_image[:, :, ::-1]
            height, width = original_image.shape[:2]
            image = original_image
            image = torch.as_tensor(image.astype("float32").transpose(2, 0, 1))

            instance = {"image": image, "height": height, "width": width}

            inputs.append(instance)

        with torch.no_grad():
            predictions = self.model(inputs)
            return predictions
