BASE_TASK_CONFIG_PATH: challenge_objectnav2022.local.rgbd.yaml

AGENT_GPU_IDS: [0]        # IDs of GPUs to use for agent with DistributedDataParallel â€” each GPU corresponds to one agent process
SIMULATOR_GPU_IDS: [0, 1] # IDs of GPUs to use for vectorized environments
NO_GPU: 0                 # 1: ignore IDs above and run on CPU, 0: run on GPUs with IDs above
NUM_ENVIRONMENTS: 1       # number of environments (per agent process)
DUMP_LOCATION: data       # path to dump models and log
EXP_NAME: test            # experiment name
MIXED_PRECISION: 0        # 1: mixed precision inference, 0: fp32
VISUALIZE: 0              # 1: render observation and predicted semantic map, 0: no visualization
PRINT_IMAGES: 1           # 1: save visualization as images, 0: no image saving

EVAL_VECTORIZED:
  specific_episodes: 0      # 1: eval on specific episodes (for debugging), 0: eval on split normally
  specific_category: 0      # "chair": eval only on chair episodes, 0: eval on all episodes
  split: val                # eval split
  num_episodes_per_env: 100 # number of eval episodes per environment
  record_videos: 0          # 1: record videos from printed images, 0: don't
  record_planner_videos: 0  # 1: record planner videos (if record videos), 0: don't

TRAIN:
  RL:
    stop_timesteps: 1000000    # number of timesteps to train for
    lr: 0.001                  # learning rate
    num_workers: 1             # number of parallel workers
    intrinsic_rew_coeff: 0.02  # intrinsic reward (explored area) coefficient

ENVIRONMENT:
  num_sem_categories: 16  # number of semantic segmentation categories
  turn_angle: 30.0        # agent turn angle (in degrees)
  frame_height: 480       # first-person frame height (in pixels)
  frame_width: 640        # first-person frame width (in pixels)
  camera_height: 0.88     # camera sensor height (in metres)
  hfov: 79.0              # horizontal field of view (in degrees)
  min_depth: 0.5          # minimum depth for depth sensor (in metres)
  max_depth: 5.0          # maximum depth for depth sensor (in metres)

AGENT:
  max_steps: 498          # maximum number of steps before stopping an episode
  panorama_start: 1       # 1: turn around 360 degrees when starting an episode, 0: don't

  SEMANTIC_MAP:
    map_size_cm: 4800        # global map size (in centimetres)
    map_resolution: 5        # size of map bins (in centimeters)
    vision_range: 100        # diameter of local map region visible by the agent (in cells)
    global_downscaling: 2    # ratio of global over local map
    du_scale: 4              # frame downscaling before projecting to point cloud
    cat_pred_threshold: 5.0  # number of depth points to be in bin to classify it as a certain semantic category
    exp_pred_threshold: 1.0  # number of depth points to be in bin to consider it as explored
    map_pred_threshold: 1.0  # number of depth points to be in bin to consider it as obstacle

  PLANNER:
    collision_threshold: 0.20       # distance under which we consider there's a collision
    obs_denoise_selem_radius: 0     # radius (in cells) of obstacle noise removal structuring element
    obs_dilation_selem_radius: 3    # radius (in cells) of obstacle dilation structuring element

  POLICY:
    type: frontier             # one of "frontier" or "semantic"
    hint_follow_steps: 15      # how many steps to follow a hint of the object goal category
    hint_in_frame: 1           # 1: look for hint in frame, 0: don't

    SEMANTIC:
      goal_update_steps: 25    # frequency at which to update the high-level goal during inference
      training_downscaling: 2  # ratio of map size in this code over map size used to train the policy
